{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dbee66-fef9-40c3-8294-8614dc44e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Paths\n",
    "yolo_base = pathlib.Path(\"data_yolo\")\n",
    "image_files = sorted((yolo_base / \"images\").glob(\"*/*.jpg\"))\n",
    "label_files = sorted((yolo_base / \"labels\").glob(\"*/*.txt\"))\n",
    "\n",
    "# Transformations\n",
    "transform = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(640, 640), scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "    v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomRotation(degrees=90)\n",
    "])\n",
    "\n",
    "# Store results if needed\n",
    "all_transformed_images = []\n",
    "all_transformed_bboxes = []\n",
    "all_classes = []\n",
    "all_batch_idx = []\n",
    "\n",
    "# Process all images\n",
    "for i, (img_path, label_path) in enumerate(zip(image_files, label_files)):\n",
    "    # Load image\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = v2.ToImage()(img)\n",
    "\n",
    "    # Load labels\n",
    "    with open(label_path) as f:\n",
    "        label_data = [line.strip().split() for line in f.readlines()]\n",
    "\n",
    "    if not label_data:\n",
    "        continue  # skip empty labels\n",
    "\n",
    "    classes = torch.tensor([int(row[0]) for row in label_data])\n",
    "    bboxes = torch.tensor([[float(x) for x in row[1:]] for row in label_data])\n",
    "\n",
    "    # Apply transform (single image version)\n",
    "    transformed_img, transformed_bboxes = transform(img_tensor.unsqueeze(0), bboxes)\n",
    "    transformed_img = transformed_img.squeeze(0)\n",
    "\n",
    "    # Store (optional)\n",
    "    all_transformed_images.append(transformed_img)\n",
    "    all_transformed_bboxes.append(transformed_bboxes)\n",
    "    all_classes.append(classes)\n",
    "    all_batch_idx.extend([i] * len(bboxes))\n",
    "\n",
    "# If needed: stack all into batch tensors\n",
    "all_images_tensor = torch.stack(all_transformed_images)\n",
    "all_bboxes_tensor = torch.cat(all_transformed_bboxes)\n",
    "all_classes_tensor = torch.cat(all_classes)\n",
    "all_batch_idx_tensor = torch.tensor(all_batch_idx)\n",
    "\n",
    "# Example: Visualize first transformed image\n",
    "plot_with_bboxes(all_images_tensor, all_bboxes_tensor, all_classes_tensor, batch_idx=all_batch_idx_tensor, index=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
